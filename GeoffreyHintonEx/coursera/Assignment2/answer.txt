

2. Train a model with 50 dimensional embedding space, 200 dimensional hidden layer 
and default setting of all other hyperparameters. What is average training set 
cross entropy as reported by the training program after 10 epochs ? Please provide 
a numeric answer (three decimal places). [4 points]

2.535

3. Train a model for 10 epochs with a 50 dimensional embedding space, 200 
dimensional hidden layer, a learning rate of 100.0 and default setting of all other
hyperparameters. What do you observe ? [3 points]

Cross Entropy on the validation set fluctuates around a large value.
Cross Entropy on the training set fluctuates around a large value.


4. If all weights and biases in this network were set to zero and no training
is performed, what will be the average cross entropy on the training set ? 
Please provide a numeric answer (three decimal places). [3 points]

*If all weights and biases are zero, the output distribution will be uniform for all 
inputs. The entropy will then be log(n) where n is the number of words in the 
vocabulary. In this case it will log(250)

5. Train three models each with 50 dimensional embedding space, 200 dimensional hidden layer.

Model A: Learning rate = 0.001, (4.431)
Model B: Learning rate = 0.1,  (3.974)
Model C: Learning rate = 10.0. (4.707)

Use the default settings for all other hyperparameters. Which model gives the lowest 
training set cross entropy after 1 epoch ? [3 points]

Model B


6. In the models trained in Question 5, which one gives the lowest training set
cross entropy after 10 epochs ? [2 points]

model = train(10, 0.001); (4.378)
model = train(10, 0.1);   (~2.5)
model = train(10, 10);	  (no need to run)

Model B


7. Train each of following models:

Model A: 5 dimensional embedding, 100 dimensional hidden layer      (2.607)
Model B: 50 dimensional embedding, 10 dimensional hidden layer      (3.009)
Model C: 50 dimensional embedding, 200 dimensional hidden layer     (2.534)
Model D: 100 dimensional embedding, 5 dimensional hidden layer      (3.238)
Use default values for all other hyperparameters.

Which model gives the best training set cross entropy after 10 epochs of training ? [3 points]

Model C

8. In the models trained in Question 7, which one gives the best validation set 
cross entropy after 10 epochs of training ? [2 points]

Model A: 5 dimensional embedding, 100 dimensional hidden layer      (~2.61)
Model B: 50 dimensional embedding, 10 dimensional hidden layer      (3.019)
Model C: 50 dimensional embedding, 200 dimensional hidden layer     (2.602)
Model D: 100 dimensional embedding, 5 dimensional hidden layer      (3.246)

Model C


9. Train three models each with 50 dimensional embedding space, 200 dimensional hidden layer.

Model A: Momentum = 0.0         (3.276, 3.245)
Model B: Momentum = 0.5	        (2.938, 2.929)
Model C: Momentum = 0.9         (2.534, 2.602)
Use the default settings for all other hyperparameters. Which model gives the lowest 
validation set cross entropy after 5 epochs ? [3 points]

Model C

10. Train a model with 50 dimensional embedding layer and 200 dimensional hidden layer for
10 epochs. Use default values for all other hyperparameters.

Which words are among the 10 closest words to the word 'day'. [2 points]

>> display_nearest_words('day', model, 10);
night 1.79
week 2.07
days 2.29
year 2.48
season 2.77
game 2.79
case 2.95
years 2.98
times 3.07
yesterday 3.07




11. In the model trained in Question 10, why is the word 'percent' close to 'dr.' 
even though they have very different contexts and are not expected to be close in 
word embedding space? [2 points]

Both words occur very rarely, so their embedding weights get updated very few times 
and remain close to their initialization.


12. In the model trained in Question 10, why is 'he' close to 'she' even though they 
refer to completely different genders? [2 points]

The model does not care about gender. It puts them close because if 'he' occurs in 
a 4-gram, it is very likely that substituting it by 'she' will also make a sensible 4-gram.


13. In conclusion, what kind of words does the model put close to each other in embedding 
space. Choose the most appropriate answer. [3 points]

Words that can be substituted for one another and still make up a sensible 4-gram.



